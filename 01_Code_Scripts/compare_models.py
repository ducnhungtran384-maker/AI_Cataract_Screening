import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os
import numpy as np
import json
import random
import time

# 1. é…ç½®æ¨¡å‹ä¸‹è½½è·¯å¾„ (å¿…é¡»åœ¨ import torchvision å‰è®¾ç½®ï¼Œæˆ–å°½æ—©è®¾ç½®)
os.environ['TORCH_HOME'] = r"C:\Users\weirui\Desktop\AI_Test\03result"

# ================= é…ç½®åŒº =================
# 1. æ•°æ®é›†è·¯å¾„
DATA_ROOT = r"C:\Users\weirui\Desktop\AI_Test\02data\ALL_Data_split12"
TRAIN_DIR = os.path.join(DATA_ROOT, "Train")
TEST_DIR = os.path.join(DATA_ROOT, "Test")

# 2. è®­ç»ƒé…ç½®
EPOCHS = 3  # GPUè·‘å…¨é‡å¯ä»¥å¤šè·‘å‡ è½®
BATCH_SIZE = 32 # GPUå¯ä»¥åŠ å¤§Batch Size
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

if DEVICE.type == 'cpu':
    print("âš ï¸  Warning: Still running on CPU!")
else:
    print(f"âœ…  Successful: Running on GPU ({torch.cuda.get_device_name(0)})")



# 3. è¾“å‡ºæ–‡ä»¶
RESULT_JS = "04visualization/js/model_comparison.js"
RESULT_PLOT = "04visualization/comparison_result.png"
# ==========================================

def get_model(model_name, num_classes=2):
    """å·¥å‚å‡½æ•°ï¼šæ ¹æ®åç§°æ„å»ºæ¨¡å‹"""
    print(f"ğŸ“¦ Building model: {model_name}...")
    
    if model_name == "resnet18":
        model = models.resnet18(pretrained=True)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, num_classes)
        
    elif model_name == "vgg16":
        model = models.vgg16(pretrained=True)
        num_ftrs = model.classifier[6].in_features
        model.classifier[6] = nn.Linear(num_ftrs, num_classes)
        
    elif model_name == "densenet121":
        model = models.densenet121(pretrained=True)
        num_ftrs = model.classifier.in_features
        model.classifier = nn.Linear(num_ftrs, num_classes)
        
    else:
        raise ValueError(f"Unknown model name: {model_name}")
        
    return model.to(DEVICE)

def calculate_metrics(tp, tn, fp, fn):
    """è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡"""
    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-10)
    precision = tp / (tp + fp + 1e-10)
    recall = tp / (tp + fn + 1e-10)  # Sensitivity
    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)
    specificity = tn / (tn + fp + 1e-10)
    return {
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1": float(f1),
        "specificity": float(specificity)
    }

def calculate_confidence_distribution(confidences):
    """
    è®¡ç®—ç½®ä¿¡åº¦åˆ†å¸ƒ
    Bins: <0.5 (unused), 0.5-0.6, 0.6-0.7, 0.7-0.8, 0.8-0.9, 0.9-1.0
    Returns: Array of counts, length 6
    """
    bins = [0] * 6
    for c in confidences:
        if c < 0.5: bins[0] += 1
        elif c < 0.6: bins[1] += 1
        elif c < 0.7: bins[2] += 1
        elif c < 0.8: bins[3] += 1
        elif c < 0.9: bins[4] += 1
        else: bins[5] += 1
    return bins

def train_and_evaluate(model_name):
    """è®­ç»ƒå¹¶è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*40}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {model_name}")
    print(f"{'='*40}")
    
    # 1. æ•°æ®å‡†å¤‡
    print(f"Loading data from {DATA_ROOT}...")
    
    data_transforms = {
        'Train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'Test': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    try:
        full_train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=data_transforms['Train'])
        full_test_dataset = datasets.ImageFolder(TEST_DIR, transform=data_transforms['Test'])
        classes = full_train_dataset.classes
        print(f"Dataset classes: {classes}")
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return

    # --- FULL DATA MODE (No Subsetting) ---
    print("ğŸš€  å·²åˆ‡æ¢è‡³å…¨é‡æ•°æ®æ¨¡å¼ (Full Data Mode)...")
    train_dataset = full_train_dataset
    val_dataset = full_test_dataset
    
    # å¢åŠ  num_workers åŠ é€Ÿæ•°æ®è¯»å–
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    train_size = len(train_dataset)
    val_size = len(val_dataset)
    print(f"Full Dataset sizes: Train={train_size}, Val={val_size}")
    # -----------------------------------------------
    
    # 2. æ¨¡å‹ä¸ä¼˜åŒ–å™¨
    model = get_model(model_name)
    criterion = nn.CrossEntropyLoss()
    
    # Feature Extracting æ¨¡å¼ï¼šä»…è®­ç»ƒæœ€åå±‚
    params_to_update = []
    for name, param in model.named_parameters():
        if param.requires_grad == True:
            params_to_update.append(param)
            
    optimizer = optim.Adam(params_to_update, lr=LEARNING_RATE)
    
    # 3. è®­ç»ƒå¾ªç¯
    start_time = time.time()
    
    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            
        print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {running_loss/train_size:.4f}")

    train_time = time.time() - start_time
    
    # 4. è¯¦ç»†è¯„ä¼° (è®¡ç®— TP, TN, FP, FN åŠ ç½®ä¿¡åº¦åˆ†å¸ƒ)
    print(f"ğŸ” æ­£åœ¨è¯„ä¼° {model_name}...")
    model.eval()
    
    tp = 0; tn = 0; fp = 0; fn = 0
    all_confidences = []
    
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            outputs = model(inputs)
            
            # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
            probs = torch.softmax(outputs, dim=1)
            confidences, preds = torch.max(probs, 1)
            
            # æ”¶é›†ç½®ä¿¡åº¦
            all_confidences.extend(confidences.cpu().numpy().tolist())
            
            for p, t in zip(preds, labels):
                if t == 0:  # True Cataract
                    if p == 0: tp += 1
                    else:      fn += 1
                else:       # True Normal
                    if p == 1: tn += 1
                    else:      fp += 1
                    
    metrics = calculate_metrics(tp, tn, fp, fn)
    conf_dist = calculate_confidence_distribution(all_confidences)
    avg_conf = sum(all_confidences) / len(all_confidences) if all_confidences else 0
    
    print(f"âœ… {model_name} å®Œæˆ! Acc: {metrics['accuracy']:.2%} | Avg Conf: {avg_conf:.2f}")
    
    result = {
        "overall": {
            "accuracy": metrics['accuracy'],
            "precision": metrics['precision'],
            "recall": metrics['recall'],
            "f1": metrics['f1'],
            "specificity": metrics['specificity'],
            "avg_confidence": float(avg_conf),
            "confidence_distribution": conf_dist,
            "confusion_matrix": {"TP": tp, "TN": tn, "FP": fp, "FN": fn}
        },
        # ç®€å•æ¨¡æ‹Ÿåˆ†ç±»åˆ«æ€§èƒ½ï¼Œé€šå¸¸å’Œoverallæ¥è¿‘
        "cataract": { 
            "accuracy": metrics['recall'], 
            "precision": metrics['precision'], 
            "recall": metrics['recall'], 
            "f1": metrics['f1'],
            "avg_confidence": float(avg_conf) # ç®€åŒ–
        },
        "normal":   { 
            "accuracy": metrics['specificity'], 
            "precision": 0.0, 
            "recall": metrics['specificity'], 
            "f1": 0.0,
            "avg_confidence": float(avg_conf) # ç®€åŒ–
        }
    }
    
    meta = {
        "name": model_name,
        "time": train_time,
        "params": sum(p.numel() for p in model.parameters())
    }
    
    return result, meta

def main():
    if not os.path.exists("04visualization/js"):
        os.makedirs("04visualization/js", exist_ok=True)
        
    models_to_run = ["resnet18", "vgg16", "densenet121"]
    
    full_data = {}      # ç”¨äº data.js æ ¼å¼
    chart_data = {"models": [], "accuracy": [], "time": [], "params": []} # ç”¨äº comparison_result.png
    
    print(f"å¯¹æ¯”å®éªŒå¼€å§‹... è®¾å¤‡: {DEVICE}")
    
    for m in models_to_run:
        try:
            res_data, res_meta = train_and_evaluate(m)
            if res_data:
                # é”®åé¦–å­—æ¯å¤§å†™ä½œä¸ºæ˜¾ç¤ºå
                if m == "resnet18": DisplayName = "ResNet18 (Standard)"
                elif m == "vgg16": DisplayName = "VGG16"
                elif m == "densenet121": DisplayName = "DenseNet121"
                else: DisplayName = m
                
                full_data[DisplayName] = res_data
                
                chart_data["models"].append(DisplayName)
                chart_data["accuracy"].append(round(res_data["overall"]["accuracy"], 4))
                chart_data["time"].append(round(res_meta["time"], 1))
                chart_data["params"].append(round(res_meta["params"]/1e6, 1))
        except Exception as e:
            print(f"âŒ {m} å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
    
    # 5. å¯¼å‡º JS
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    
    js_content = f"""
// Auto-generated by compare_models.py
(function() {{
    const NEW_MODELS = {json.dumps(full_data, indent=4)};
    
    if (typeof MODEL_DATA !== 'undefined') {{
        console.log('Merging new comparison models into MODEL_DATA...');
        Object.assign(MODEL_DATA, NEW_MODELS);
    }} else {{
        window.MODEL_DATA = NEW_MODELS;
    }}
    
    // åŒæ—¶ä¹Ÿç”Ÿæˆç®€å•çš„å¯¹æ¯”æ•°æ®ä¾›å‚è€ƒ
    window.MODEL_COMPARISON_SIMPLE = {json.dumps(chart_data, indent=4)};
}})();
"""
    with open(RESULT_JS, 'w', encoding='utf-8') as f:
        f.write(js_content)
    print(f"âœ… JSæ•°æ®å·²ä¿å­˜è‡³: {RESULT_JS}")

    # 6. ç”ŸæˆPNGå¯¹æ¯”å›¾
    if len(chart_data["models"]) > 0:
        plt.figure(figsize=(10, 5))
        
        # Accuracy
        plt.subplot(1, 2, 1)
        bars = plt.bar(chart_data['models'], chart_data['accuracy'], color=['#3498db', '#e74c3c', '#2ecc71'])
        plt.ylim(0, 1.1)
        plt.title('Accuracy Comparison')
        for bar in bars:
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(), f'{bar.get_height():.2%}', ha='center', va='bottom')
            
        # Time
        plt.subplot(1, 2, 2)
        bars = plt.bar(chart_data['models'], chart_data['time'], color=['#3498db', '#e74c3c', '#2ecc71'])
        plt.title('Training Time (s)')
        for bar in bars:
            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(), f'{bar.get_height():.1f}s', ha='center', va='bottom')
            
        plt.tight_layout()
        plt.savefig(RESULT_PLOT)
        print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {RESULT_PLOT}")

if __name__ == "__main__":
    main()
