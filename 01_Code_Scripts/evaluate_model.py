import torch
import torch.nn as nn
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import os
import shutil
import json
import numpy as np
from sklearn.metrics import confusion_matrix
import hashlib

# ================= é…ç½®åŒº =================
# 1. æ¨¡å‹è·¯å¾„ (ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„)
MODEL_PATH = "../result/best_cataract_model.pth"

# 2. æ•°æ®é›†è·¯å¾„ (ç”¨æˆ·ç¡®è®¤çš„æµ‹è¯•é›†ä½ç½®)
DATA_PATH = "../04data/ALL_Data_split12/Test"

# 3. é”™è¯¯å›¾ç‰‡è¾“å‡ºç›®å½• (å¯¹æ¥å¤§å±ç”Ÿæˆå™¨)
ERROR_IMG_DIR = "../visualization/error_images"

# 4. JSON æ•°æ®è¾“å‡ºä½ç½® (æˆ‘ä»¬è¦å»ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶)
JS_DATA_FILE = "../visualization/js/data.js"
NEW_MODEL_NAME = "PyTorch_ResNet"
# ==========================================

def get_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶MD5ï¼Œé˜²é‡å¤"""
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

def evaluate_and_export():
    print("ğŸš€ Starting model evaluation...")
    
    # --- 1. Device setup ---
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"   Device: {device}")
    
    # Ensure error image directory exists
    if not os.path.exists(ERROR_IMG_DIR):
        os.makedirs(ERROR_IMG_DIR)
        print(f"   Created directory: {ERROR_IMG_DIR}")

    # --- 2. Prepare data ---
    if not os.path.exists(DATA_PATH):
        print(f"âŒ Error: Dataset path not found: {DATA_PATH}")
        return

    data_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    dataset = datasets.ImageFolder(DATA_PATH, transform=data_transforms)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=False) # Batch inference is faster
    print(f"   Dataset: {len(dataset)} images")
    print(f"   Classes: {dataset.class_to_idx}")

    # --- 3. Load model ---
    print("   Loading model...")
    model = models.resnet18(weights=None)
    # å¿…é¡»å…ˆä¿®æ”¹ fc å±‚æ¶æ„ä»¥åŒ¹é…ä¿å­˜çš„æ¨¡å‹ï¼ˆ2åˆ†ç±»è€Œé1000åˆ†ç±»ï¼‰
    # load_state_dict ä¼šç”¨ä¿å­˜çš„æƒé‡è¦†ç›–è¿™é‡Œçš„éšæœºåˆå§‹åŒ–
    model.fc = nn.Linear(model.fc.in_features, 2)
    try:
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
        model = model.to(device)
        model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        print("Model loaded successfully!")
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        return

    # --- 4. Batch inference ---
    all_preds = []
    all_labels = []
    all_probs = [] # Store confidence scores
    mismatches = []
    
    cat_probs = []
    norm_probs = []
    
    print("   Running inference...")
    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloader):
            inputs = inputs.to(device)
            outputs = model(inputs)
            
            # è®¡ç®—æ¦‚ç‡ (Softmax)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            confidence, preds = torch.max(probs, 1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(confidence.cpu().numpy())

            # åˆ†ç±»æ”¶é›†ç½®ä¿¡åº¦
            for k in range(len(labels)):
                label_idx = labels[k].item()
                conf_val = confidence[k].item()
                
                if label_idx == dataset.class_to_idx['Cataract']: 
                    cat_probs.append(conf_val)
                else:
                    norm_probs.append(conf_val)
            # è®°å½•é”™è¯¯æ ·æœ¬
            batch_start_idx = i * dataloader.batch_size
            for k in range(len(preds)):
                if preds[k] != labels[k]:
                    global_idx = batch_start_idx + k
                    img_path, _ = dataset.samples[global_idx]
                    mismatches.append({
                        'path': img_path,
                        'actual': labels[k].item(),
                        'predicted': preds[k].item(),
                        'confidence': confidence[k].item()
                    })

    # --- 5. è®¡ç®—åŒ»å­¦æŒ‡æ ‡ ---
    # æ··æ·†çŸ©é˜µ: tn, fp, fn, tp (æ³¨æ„ sklearn çš„é¡ºåº)
    # class 0: Cataract (Positive?), class 1: Normal (Negative?)
    # é€šå¸¸ ImageFolder æ˜¯æŒ‰å­—æ¯æ’åº: Cataract (0), Normal (1)
    # å‡è®¾ 0:Cataract æ˜¯é˜³æ€§, 1:Normal æ˜¯é˜´æ€§
    # æ··æ·†çŸ©é˜µ [[TP, FN], [FP, TN]] å–å†³äºæ ‡ç­¾å®šä¹‰
    # è®©æˆ‘ä»¬æ˜ç¡®å®šä¹‰: Target=0(Cataract) is Positive. 
    
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)
    
    # SKLearn confusion matrix
    cm = confusion_matrix(all_labels, all_preds) 
    # Label 0 (Cataract), Label 1 (Normal)
    # [ [True 0, False 1], 
    #   [False 0, True 1] ]
    # å³: [[TP, FN], [FP, TN]] (å¦‚æœ 0 æ˜¯ Positive)
    
    tp = cm[0, 0] # çœŸå®æ˜¯0ï¼Œé¢„æµ‹æ˜¯0
    fn = cm[0, 1] # çœŸå®æ˜¯0ï¼Œé¢„æµ‹æ˜¯1 (æ¼è¯Š)
    fp = cm[1, 0] # çœŸå®æ˜¯1ï¼Œé¢„æµ‹æ˜¯0 (è¯¯è¯Š)
    tn = cm[1, 1] # çœŸå®æ˜¯1ï¼Œé¢„æµ‹æ˜¯1
    
    total = np.sum(cm)
    accuracy = (tp + tn) / total
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0 # æ•æ„Ÿåº¦ Sensitivity
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 # ç‰¹å¼‚åº¦
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # 6. å¤„ç†é”™è¯¯å›¾ç‰‡ (å¤åˆ¶å¹¶å»é‡)
    print(f"   Found {len(mismatches)} error samples, processing images...")
    if not os.path.exists(ERROR_IMG_DIR):
         os.makedirs(ERROR_IMG_DIR)

    saved_error_images = []
    
    for m in mismatches:
        src_path = m['path']
        filename = os.path.basename(src_path)
        
        # æ·»åŠ ç±»åˆ«å‰ç¼€ä»¥é¿å…åŒåå†²çªï¼ˆNormal/3660.jpg vs Cataract/3660.jpgï¼‰
        # ä»è·¯å¾„ä¸­æå–çœŸå®ç±»åˆ«
        parent_dir = os.path.basename(os.path.dirname(src_path))
        prefixed_filename = f"{parent_dir.lower()}_{filename}"
        
        dst_path = os.path.join(ERROR_IMG_DIR, prefixed_filename)
        shutil.copy2(src_path, dst_path)
        saved_error_images.append(prefixed_filename)

    # 7. æ„å»º JSON ç»“æ„
    # ç½®ä¿¡åº¦åˆ†å¸ƒ (Histogram, 6 bins like 0.5-0.6, ..., 0.9-1.0)
    # åŸå§‹å¯è§†ä¸º [0, 6, 7, 4, 15, 602] è¿™ç§è®¡æ•°
    # æˆ‘ä»¬ç®€å•å°†ç½®ä¿¡åº¦åˆ†æ¡¶
    conf_bins = [0] * 6 # <0.6, 0.6-0.7, 0.7-0.8, 0.8-0.9, 0.9-0.95, >0.95
    for p in all_probs:
        if p < 0.6: conf_bins[0] += 1
        elif p < 0.7: conf_bins[1] += 1
        elif p < 0.8: conf_bins[2] += 1
        elif p < 0.9: conf_bins[3] += 1
        elif p < 0.95: conf_bins[4] += 1
        else: conf_bins[5] += 1

    model_json = {
        "overall": {
            "accuracy": round(accuracy, 4),
            "precision": round(precision, 4),
            "recall": round(recall, 4),
            "f1": round(f1, 4),
            "specificity": round(specificity, 4),
            "confusion_matrix": {
                "TP": int(tp), "TN": int(tn), "FP": int(fp), "FN": int(fn)
            },
            "avg_confidence": round(float(np.mean(all_probs)), 4),
            "total": int(total),
            "confidence_distribution": conf_bins
        },
        "cataract": {
            "precision": round(tp / (tp+fp) if (tp+fp)>0 else 0, 4),
            "recall": round(tp / (tp+fn) if (tp+fn)>0 else 0, 4),
            "accuracy": round(tp / (tp+fn) if (tp+fn)>0 else 0, 4), # Class accuracy = Recall (TP/Actual Positives)
            "f1": round(2 * (tp / (tp+fp)) * (tp / (tp+fn)) / ((tp / (tp+fp)) + (tp / (tp+fn))) if ((tp / (tp+fp)) + (tp / (tp+fn))) > 0 else 0, 4),
            "count": int(tp+fn),
            "correct": int(tp),
            "avg_confidence": round(float(np.mean(cat_probs)) if cat_probs else 0, 4)
        },
        "normal": {
            "precision": round(tn / (tn+fn) if (tn+fn)>0 else 0, 4),
            "recall": round(tn / (tn+fp) if (tn+fp)>0 else 0, 4),
            "accuracy": round(tn / (tn+fp) if (tn+fp)>0 else 0, 4), # Class accuracy = Recall (TN/Actual Negatives)
            "f1": round(2 * (tn / (tn+fn)) * (tn / (tn+fp)) / ((tn / (tn+fn)) + (tn / (tn+fp))) if ((tn / (tn+fn)) + (tn / (tn+fp))) > 0 else 0, 4),
            "count": int(tn+fp),
            "correct": int(tn),
            "avg_confidence": round(float(np.mean(norm_probs)) if norm_probs else 0, 4)
        }
    }
    
    print("\nâœ… Metrics calculation complete:")
    print(f"   Accuracy: {accuracy:.2%}")
    print(f"   F1 Score: {f1:.4f}")
    
    # 8. å¯¼å‡ºé”™è¯¯æ¡ˆä¾‹åˆ° error_data.js
    export_error_cases(mismatches, dataset.class_to_idx)
    
    # 9. æ³¨å…¥åˆ° data.js
    # è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒæš´åŠ›çš„æ–‡æœ¬æ›¿æ¢ï¼Œä½†æœ€æœ‰æ•ˆ
    inject_to_js(model_json)

def export_error_cases(mismatches, class_to_idx):
    """å¯¼å‡ºé”™è¯¯æ¡ˆä¾‹åˆ° error_data.js"""
    print(f"\nğŸ“Š Exporting {len(mismatches)} error cases to error_data.js...")
    
    # åè½¬ class_to_idx ä»¥ä¾¿ä»ç´¢å¼•è·å–ç±»å
    idx_to_class = {v: k for k, v in class_to_idx.items()}
    
    # æ„å»ºé”™è¯¯æ¡ˆä¾‹åˆ—è¡¨
    error_cases = []
    for m in mismatches:
        filename = os.path.basename(m['path'])
        
        # æ·»åŠ ç±»åˆ«å‰ç¼€
        parent_dir = os.path.basename(os.path.dirname(m['path']))
        prefixed_filename = f"{parent_dir.lower()}_{filename}"
        
        error_cases.append({
            "filename": prefixed_filename,
            "true_label": idx_to_class[m['actual']],
            "pred_label": idx_to_class[m['predicted']],
            "confidence": round(m['confidence'], 8),
            "image_path": f"error_images/{prefixed_filename}"
        })
    
    # ç”Ÿæˆ JavaScript ä»£ç 
    js_content = "const ERROR_CASES = [\n"
    for i, case in enumerate(error_cases):
        js_content += "  {\n"
        js_content += f'    "filename": "{case["filename"]}",\n'
        js_content += f'    "true_label": "{case["true_label"]}",\n'
        js_content += f'    "pred_label": "{case["pred_label"]}",\n'
        js_content += f'    "confidence": {case["confidence"]},\n'
        js_content += f'    "image_path": "{case["image_path"]}"\n'
        js_content += "  }"
        if i < len(error_cases) - 1:
            js_content += ","
        js_content += "\n"
    js_content += "];\n"
    
    # å†™å…¥æ–‡ä»¶
    error_data_path = "../visualization/js/error_data.js"
    with open(error_data_path, 'w', encoding='utf-8') as f:
        f.write(js_content)
    
    print(f"âœ… Exported {len(error_cases)} error cases to {error_data_path}")
    return error_cases

def inject_to_js(json_data):
    print("\nâœï¸ Injecting data into data.js...")
    
    with open(JS_DATA_FILE, 'r', encoding='utf-8') as f:
        content = f.read()

    # å°† JSON å¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œä½†è¦å»æ‰æœ€å¤–å±‚çš„èŠ±æ‹¬å·ï¼Œä»¥ä¾¿å¡è¿›å»
    # æˆ–è€…æˆ‘ä»¬ç›´æ¥æ‰¾ const MODEL_DATA = { åé¢çš„ä½ç½®æ’å…¥
    
    json_str = json.dumps(json_data, indent=4, ensure_ascii=False)
    # æˆ‘ä»¬æ„é€ ä¸€ä¸ª key: value å­—ç¬¦ä¸²ï¼Œæ³¨æ„ç¼©è¿›
    # key ä¸éœ€è¦å¼•å·? JSé‡Œè¿˜æ˜¯éœ€è¦çš„æœ€å¥½
    insertion = f'  "{NEW_MODEL_NAME}": {json_str},\n'
    
    # å¯»æ‰¾æ’å…¥ç‚¹ï¼šMODEL_DATA = { ä¹‹å
    marker = "const MODEL_DATA = {"
    pos = content.find(marker)
    if pos == -1:
        print("âŒ Cannot find insertion point in data.js")
        return
        
    # æ’å…¥
    new_content = content[:pos + len(marker)] + "\n" + insertion + content[pos + len(marker):]
    
    # è¿˜éœ€è¦æ›´æ–° MODEL_NAMES åˆ—è¡¨
    # æ‰¾åˆ° const MODEL_NAMES = [
    marker_names = "const MODEL_NAMES = ["
    pos_names = new_content.find(marker_names)
    if pos_names != -1:
        # åœ¨ [ åé¢æ’å…¥ "PyTorch_ResNet", 
        new_content = new_content[:pos_names + len(marker_names)] + f'"{NEW_MODEL_NAME}", ' + new_content[pos_names + len(marker_names):]
    
    # å†™å…¥
    with open(JS_DATA_FILE, 'w', encoding='utf-8') as f:
        f.write(new_content)
        
    print("âœ… Data injection successful!")

if __name__ == "__main__":
    evaluate_and_export()
