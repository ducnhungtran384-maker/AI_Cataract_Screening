import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os
import numpy as np

# ==========================================
# é˜Ÿå‹ä¸“ç”¨ï¼šç™½å†…éšœ AI è®­ç»ƒæ¨¡æ¿ (PyTorch ç‰ˆ)
# ==========================================

# 1. åŸºç¡€é…ç½® (åœ¨è¿™é‡Œä¿®æ”¹è·¯å¾„)
DATA_PATH = "Split_Data/Split_Data/Train"  # è¯·ç¡®ä¿æ–‡ä»¶å¤¹å†…æœ‰ 'cataract' å’Œ 'normal' ä¸¤ä¸ªå­æ–‡ä»¶å¤¹
BATCH_SIZE = 16
EPOCHS = 10
LEARNING_RATE = 0.001
SAVE_PATH = "best_cataract_model.pth"

def train_model():
    print(f"æ­£åœ¨å‡†å¤‡æ•°æ®...")

    # 2. æ•°æ®å¢å¼º (ä½ çš„åŠŸåŠ³ç‚¹ï¼šåŒ»ç–—å½±åƒä¼˜åŒ–)
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.1, contrast=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    # åŠ è½½æ•°æ®é›†
    if not os.path.exists(DATA_PATH):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è·¯å¾„ {DATA_PATH}ï¼Œè¯·å…ˆå‡†å¤‡å¥½æ•°æ®é›†æ–‡ä»¶å¤¹ï¼")
        return

    full_dataset = datasets.ImageFolder(DATA_PATH, data_transforms['train'])

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    print(f"æ•°æ®é›†è½½å…¥æˆåŠŸï¼šè®­ç»ƒé›† {train_size} å¼ ï¼ŒéªŒè¯é›† {val_size} å¼ ")
    print(f"æ ‡ç­¾å¯¹åº”å…³ç³»: {full_dataset.class_to_idx}")

    # 3. æ„å»ºæ¨¡å‹ (è®¾å¤‡è‡ªåŠ¨é€‰æ‹©)
    if torch.cuda.is_available():
        device = torch.device("cuda:0")
        print(f"ğŸš€ [æ£€æµ‹æˆåŠŸ] å·²æˆåŠŸè°ƒç”¨ NVIDIA æ˜¾å¡è¿›è¡ŒåŠ é€Ÿè®­ç»ƒï¼")
    else:
        device = torch.device("cpu")
        print(f"â„¹ï¸ [æ£€æµ‹æç¤º] æœªæ£€æµ‹åˆ°æ˜¾å¡åŠ é€Ÿåº“ï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒã€‚")
        print(f"   (æ³¨ï¼šè‹¥è¦å¼€å¯æ˜¾å¡åŠ é€Ÿï¼Œéœ€å®‰è£… 2.8G çš„ CUDA é©±åŠ¨åŒ…ï¼Œä½†è¿™å¹¶éå¿…é¡»ï¼ŒCPU ä¹Ÿèƒ½å®Œæˆä»»åŠ¡)")

    print(f"å½“å‰è¿è¡Œè®¾å¤‡: {device}")

    # ä½¿ç”¨ ResNet18 é¢„è®­ç»ƒæ¨¡å‹
    model = models.resnet18(pretrained=True)
    num_ftrs = model.fc.in_features
    # ä¿®æ”¹è¾“å‡ºå±‚ä¸º 2 ç±» (Cataract vs Normal)
    model.fc = nn.Linear(num_ftrs, 2)
    model = model.to(device)

    # 4. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 5. å¼€å§‹è®­ç»ƒ
    print("\n--- è®­ç»ƒæ­£å¼å¼€å§‹ ---")
    best_acc = 0.0

    # è®°å½•æ•°æ®ç”¨äºç”»å›¾
    history = {'train_loss': [], 'val_acc': []}

    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / train_size
        history['train_loss'].append(epoch_loss)

        # éªŒè¯æ¨¡å‹
        model.eval()
        correct = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                correct += torch.sum(preds == labels.data)

        epoch_acc = correct.double() / val_size
        history['val_acc'].append(epoch_acc.item())

        print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss:.4f} | Val Acc: {epoch_acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_acc > best_acc:
            best_acc = epoch_acc
            torch.save(model.state_dict(), SAVE_PATH)

    print(f"\nè®­ç»ƒè¿è¡Œå®Œæ¯•ï¼æœ€é«˜å‡†ç¡®ç‡: {best_acc:.4f}")
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {SAVE_PATH}")

    # 6. å…¨æ–¹ä½è¯„ä¼° (ä½ çš„åŠŸåŠ³ç‚¹ï¼šæ·±åº¦æ¨¡å‹åˆ†æ)
    print("\n--- æ­£åœ¨ç”Ÿæˆä»ªè¡¨ç›˜æ•°æ®åŒ… ---")
    model.eval()
    all_preds = []
    all_labels = []
    all_confs = []

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            confs, preds = torch.max(probs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_confs.extend(confs.cpu().numpy())

    # è®¡ç®—å„é¡¹æŒ‡æ ‡ (æ‰‹åŠ¨å®ç°ä»¥å‡å°‘åº“ä¾èµ–)
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_confs = np.array(all_confs)

    tp = np.sum((all_preds == 0) & (all_labels == 0)) # Cataract ä¸º 0
    tn = np.sum((all_preds == 1) & (all_labels == 1)) # Normal ä¸º 1
    fp = np.sum((all_preds == 0) & (all_labels == 1))
    fn = np.sum((all_preds == 1) & (all_labels == 0))

    acc = (tp + tn) / len(all_labels)
    prec = tp / (tp + fp) if (tp + fp) > 0 else 0
    rec = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0
    spec = tn / (tn + fp) if (tn + fp) > 0 else 0

    # è®¡ç®—åˆ†ç±»è¯¦ç»†æŒ‡æ ‡
    cat_count = np.sum(all_labels == 0)
    norm_count = np.sum(all_labels == 1)
    cat_correct = tp
    norm_correct = tn

    # ç½®ä¿¡åº¦åˆ†å¸ƒ (6ä¸ªåˆ†æ®µ: <0.5, 0.5-0.6, 0.6-0.7, 0.7-0.8, 0.8-0.9, 0.9-1.0)
    bins = [0, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    hist, _ = np.histogram(all_confs, bins=bins)

    # æ‰“å° JSON æ•°æ®åŒ… (ä½ å¯ä»¥ç›´æ¥å¤åˆ¶åˆ° data.js ä¸­)
    print("\n" + "="*50)
    print("ğŸ“¢ å¤åˆ¶ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ° visualization/js/data.js çš„ MODEL_DATA ä¸­:")
    print("="*50)
    import json
    dashboard_data = {
        "overall": {
            "accuracy": round(float(acc), 4),
            "precision": round(float(prec), 4),
            "recall": round(float(rec), 4),
            "f1": round(float(f1), 4),
            "specificity": round(float(spec), 4),
            "confusion_matrix": {"TP": int(tp), "TN": int(tn), "FP": int(fp), "FN": int(fn)},
            "avg_confidence": round(float(np.mean(all_confs)), 4),
            "total": int(len(all_labels)),
            "confidence_distribution": hist.tolist()
        },
        "cataract": {
            "precision": round(float(prec), 4),
            "recall": round(float(rec), 4),
            "count": int(cat_count),
            "correct": int(cat_correct),
            "avg_confidence": round(float(np.mean(all_confs[all_labels == 0])), 4) if cat_count > 0 else 0
        },
        "normal": {
            "precision": round(float(tn/(tn+fn))) if (tn+fn)>0 else 0,
            "recall": round(float(tn/(tn+fp))) if (tn+fp)>0 else 0,
            "count": int(norm_count),
            "correct": int(norm_correct),
            "avg_confidence": round(float(np.mean(all_confs[all_labels == 1])), 4) if norm_count > 0 else 0
        }
    }
    print(f'  "Lead_Model": {json.dumps(dashboard_data, indent=4, ensure_ascii=False)},')
    print("="*50)

    # 7. è‡ªåŠ¨ç»˜å›¾
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Loss')
    plt.title('Training Loss')
    plt.subplot(1, 2, 2)
    plt.plot(history['val_acc'], label='Accuracy')
    plt.title('Validation Accuracy')
    plt.savefig('training_result.png')
    print("\nè®­ç»ƒç»“æœåˆ†æå›¾å·²ç”Ÿæˆ: training_result.png")

if __name__ == "__main__":
    train_model()